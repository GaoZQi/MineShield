import pickle
import numpy as np
import re
import nltk
import matplotlib.pyplot as plt
from urllib.parse import unquote
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import pandas as pd
import os
import sys
import warnings
warnings.filterwarnings("ignore")
# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.family'] = 'SimHei'
plt.rcParams['axes.unicode_minus'] = False

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# æ‹¼æ¥æ¨¡å‹å’Œ tokenizer è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„ï¼‰
model_path = os.path.join(BASE_DIR, "../../../res/model/LSTM-XSS/xss_model.h5")
tokenizer_path = os.path.join(BASE_DIR, "../../../res/model/LSTM-XSS/tokenizer-XSS.pkl")

# ----------------------------
# åŠ è½½æ¨¡å‹å’Œ tokenizer
# ----------------------------
model = load_model(model_path)
with open(tokenizer_path, "rb") as f:
    tokenizer = pickle.load(f)

# ----------------------------
# åˆ†è¯å‡½æ•°
# ----------------------------
def GeneSeg(payload):
    payload = payload.lower()
    payload = unquote(unquote(payload))
    payload = re.sub(r'\d+', "0", payload)
    payload = re.sub(r'(http|https)://[a-zA-Z0-9\.@&/#!#\?]+', "http://u", payload)
    r = r'''(?x)[\w\.]+?\(|\)|"\w+?"|'\w+?'|http://\w|</\w+>|<\w+>|<\w+|\w+=|>|[\w\.]+'''
    return nltk.regexp_tokenize(payload, r)

# ----------------------------
# è®©ç”¨æˆ·è¾“å…¥ CSV æ•°æ®è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„ï¼‰
# ----------------------------
data_file_input = input("è¯·è¾“å…¥å¸¦æ ‡ç­¾çš„ CSV æ•°æ®é›†è·¯å¾„ï¼ˆåŒ…å« id, Sentence, label ä¸‰åˆ—ï¼‰: ").strip()
data_file_path = os.path.abspath(os.path.join(os.getcwd(), data_file_input))

if not os.path.isfile(data_file_path):
    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_file_path}")
    sys.exit()

# ----------------------------
# è¯»å– CSV æ•°æ®ï¼ˆå…¼å®¹ä¸­æ–‡ç¼–ç ï¼‰
# ----------------------------
try:
    data = pd.read_csv(data_file_path)
except UnicodeDecodeError:
    print("âš ï¸ UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ gb18030 ç¼–ç è¯»å–...")
    data = pd.read_csv(data_file_path, encoding='gb18030')

# ----------------------------
# æ£€æŸ¥åˆ—æ˜¯å¦åˆæ³•
# ----------------------------
if not {'id', 'Sentence', 'label'}.issubset(data.columns):
    print("âŒ æ•°æ®é›†å¿…é¡»åŒ…å«åˆ—åï¼šid, Sentence, label")
    sys.exit()

# ----------------------------
# é¢„å¤„ç†æ•°æ®
# ----------------------------
tokens_list = [GeneSeg(sentence) for sentence in data['Sentence']]
sequences = tokenizer.texts_to_sequences(tokens_list)
padded_sequences = pad_sequences(sequences, maxlen=model.input_shape[1])

# ----------------------------
# æ¨¡å‹é¢„æµ‹
# ----------------------------
predictions = model.predict(padded_sequences, verbose=0)
predicted_labels = (predictions > 0.5).astype(int).flatten()
true_labels = data['label'].astype(int).values

# ----------------------------
# è¾“å‡ºå‰20æ¡é¢„æµ‹ç»“æœ
# ----------------------------
print("\nğŸ“¢ å‰20æ¡é¢„æµ‹ç»“æœï¼š")
for i, (idx, sentence, pred, label) in enumerate(zip(data['id'][:20], data['Sentence'][:20], predicted_labels[:20], true_labels[:20])):
    pred_text = "XSSæ”»å‡»" if pred == 1 else "æ­£å¸¸"
    real_text = "XSSæ”»å‡»" if label == 1 else "æ­£å¸¸"
    print(f"{i+1}. ID: {idx}  [é¢„æµ‹: {pred_text}]  [å®é™…: {real_text}] å†…å®¹: {sentence[:80]}...")

# ----------------------------
# æ··æ·†çŸ©é˜µ + åˆ†ç±»æŠ¥å‘Š
# ----------------------------
cm = confusion_matrix(true_labels, predicted_labels)
print("\nğŸ“„ åˆ†ç±»æŠ¥å‘Šï¼š\n")
print(classification_report(true_labels, predicted_labels, target_names=["æ­£å¸¸", "XSSæ”»å‡»"]))

plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["æ­£å¸¸", "XSSæ”»å‡»"], yticklabels=["æ­£å¸¸", "XSSæ”»å‡»"])
plt.title("æ··æ·†çŸ©é˜µ")
plt.xlabel("é¢„æµ‹æ ‡ç­¾")
plt.ylabel("çœŸå®æ ‡ç­¾")
plt.tight_layout()

# ----------------------------
# æ€»ä½“ç»Ÿè®¡
# ----------------------------
total = len(predicted_labels)
count_attack = np.sum(predicted_labels)
count_normal = total - count_attack
print(f"\nâœ… æ€»å…±é¢„æµ‹ {total} æ¡è¯·æ±‚ï¼š")
print(f"   - é¢„æµ‹ä¸ºæ­£å¸¸ï¼š{count_normal} æ¡")
print(f"   - é¢„æµ‹ä¸º XSS æ”»å‡»ï¼š{count_attack} æ¡")
