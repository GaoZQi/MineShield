import pandas as pd
import numpy as np
import math
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from urllib import parse as urlparse
import urllib
import warnings
warnings.filterwarnings("ignore")
import os
# -------- ğŸ” ç‰¹å¾æå–å‡½æ•° ----------
def extract_features(request_str):
    lower = request_str.lower()
    decoded = urllib.parse.unquote(request_str)
    parsed = urlparse.urlparse(decoded)
    url_len = len(request_str)

    # å‚æ•°è§£æ
    params = urllib.parse.parse_qs(parsed.query)
    param_lengths = [len(v[0]) if v else 0 for v in params.values()]
    avg_param_len = np.mean(param_lengths) if param_lengths else 0

    # SQLå…³é”®è¯æ£€æµ‹
    sql_keywords = ['select', 'insert', 'update', 'union', 'drop', 'script', 'iframe']
    has_sql_keyword = int(any(k in lower for k in sql_keywords))

    # æ˜¯å¦æ˜¯base64ï¼ˆç®€å•åˆ¤å®šï¼‰
    is_base64 = int(('=' in request_str) and any(part.isalnum() and len(part) > 8 for part in request_str.split()))

    # æ˜¯å¦å­˜åœ¨é‡å¤å‚æ•°
    repeated_params = int(len(params) != len(set(params)))

    # åŒé‡ç¼–ç æ£€æµ‹
    has_double_encoding = int('%25' in request_str)

    # å…¶ä»–ç‰¹å¾
    param_count = request_str.count('=')
    special_chars = ['\'', '"', '<', '>', '#', '&', '%', '`', ';', '\\']
    special_char_count = sum(request_str.count(c) for c in special_chars)
    is_encoded = 1 if request_str.count('%') >= 3 else 0
    keyword_count = sum(lower.count(k) for k in sql_keywords)
    path_depth = len(parsed.path.strip('/').split('/')) if parsed.path else 0
    digit_ratio = sum(c.isdigit() for c in request_str) / url_len
    alpha_ratio = sum(c.isalpha() for c in request_str) / url_len
    symbol_ratio = sum((not c.isalnum()) for c in request_str) / url_len

    tmp_dict = {}
    for c in request_str:
        tmp_dict[c] = tmp_dict.get(c, 0) + 1
    entropy = -sum((v / url_len) * math.log(v / url_len, 2) for v in tmp_dict.values())

    return pd.Series([
        avg_param_len, has_double_encoding, has_sql_keyword, is_base64, repeated_params,
        url_len, param_count, special_char_count, is_encoded,
        keyword_count, path_depth,
        digit_ratio, alpha_ratio, symbol_ratio, entropy
    ])

# ç‰¹å¾åé¡ºåºéœ€ä¸è®­ç»ƒä¸€è‡´
feature_names = [
    'avg_param_len', 'has_double_encoding', 'has_sql_keyword', 'is_base64', 'repeated_params',
    'len', 'param_count', 'special_char_count', 'is_encoded',
    'keyword_count', 'path_depth',
    'digit_ratio', 'alpha_ratio', 'symbol_ratio', 'entropy'
]

# -------- ğŸ“¦ é¢„æµ‹æ•°æ®é¢„å¤„ç† ----------
def preprocess_data(file, label):
    data = pd.read_csv(file, delimiter='\t', encoding='utf-8')
    features = data['request'].apply(extract_features)
    features.columns = feature_names
    features['label'] = label
    return features

# -------- ğŸ”® ä¸»é¢„æµ‹å‡½æ•° ----------
def run_predict():
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    # è¾“å…¥æ”»å‡»è¯·æ±‚æ–‡ä»¶
    abnormal_file = input("è¯·è¾“å…¥æ”»å‡»è¯·æ±‚æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ injec_.csvï¼‰ï¼š").strip()
    normal_file = os.path.join(BASE_DIR,'../../../res/data/I SVM-SQL/pass_.csv')
    model_file = os.path.join(BASE_DIR,'../../../res/model/SVC-SQL/svc_model.pkl')
    scaler_file = os.path.join(BASE_DIR,'../../../res/model/SVC-SQL/scaler.pkl')

    print("\nğŸ“¦ æ­£åœ¨è¯»å–å’Œå¤„ç†æµ‹è¯•æ•°æ®...")
    normal_data = preprocess_data(normal_file, 0)
    abnormal_data = preprocess_data(abnormal_file, 1)
    test_data = pd.concat([normal_data, abnormal_data], axis=0)

    y_true = test_data['label'].values
    X_test = test_data.drop(['label'], axis=1)

    print("âš™ï¸ åŠ è½½ scaler å¹¶è¿›è¡Œæ ‡å‡†åŒ–...")
    scaler = joblib.load(scaler_file)
    X_test = X_test[scaler.feature_names_in_]
    X_scaled = scaler.transform(X_test)

    print("ğŸ¤– åŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
    model = joblib.load(model_file)
    y_pred = model.predict(X_scaled)

    # è¾“å‡ºç»“æœ
    acc = accuracy_score(y_true, y_pred)
    print(f"\nğŸ¯ æ¨¡å‹é¢„æµ‹å‡†ç¡®ç‡ï¼š{acc:.4f}")
    print("\nğŸ“„ åˆ†ç±»æŠ¥å‘Šï¼š\n", classification_report(y_true, y_pred, target_names=['æ­£å¸¸è¯·æ±‚', 'æ”»å‡»è¯·æ±‚']))

    cm = confusion_matrix(y_true, y_pred)
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['æ­£å¸¸è¯·æ±‚ï¼ˆé¢„æµ‹ï¼‰', 'æ”»å‡»è¯·æ±‚ï¼ˆé¢„æµ‹ï¼‰'],
                yticklabels=['æ­£å¸¸è¯·æ±‚ï¼ˆå®é™…ï¼‰', 'æ”»å‡»è¯·æ±‚ï¼ˆå®é™…ï¼‰'])
    plt.xlabel('é¢„æµ‹ç»“æœ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title('é¢„æµ‹æ··æ·†çŸ©é˜µ')
    plt.tight_layout()

# -------- ğŸš€ å¯åŠ¨å…¥å£ ----------
if __name__ == '__main__':
    run_predict()
